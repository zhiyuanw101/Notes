# 机器学习
---
## pooling的做用
1. 提高感受野
2. 降低参数与计算量，防止过拟合
3. 增强平移/旋转/尺度不变性

---

## [dropout](https://zhuanlan.zhihu.com/p/38200980)
- 作用：
    1. 正则化减少过拟合
    2. 减少神经元间复杂的共适应关系：两个神经元不同时出现
- 做法：
    1. 训练时让神经元以概率p停止工作（输出0），正向/反向传播不经过这些点
    2. 测试时全部出现

---

## [Batch Normalization](https://www.zhihu.com/question/38102762)
https://www.cnblogs.com/guoyaohua/p/8724433.html
BatchNorm就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同分布的。
- motivation：
    - Internal Covariate Shift: 在训练过程中，隐层的输入分布变化
    - 让每个隐层节点的**激活输入分布**固定下来
- 作用：
    1.  减轻了对参数初始化的依赖，这是利于调参的朋友们的。
    2.  基于梯度的训练过程可以更加有效的进行，即加快收敛速度，减轻梯度消失或爆炸导致的无法训练的问题
    3.  BN一定程度上增加了泛化能力，dropout等技术可以去掉。
- 使用：
    - 在激活函数前加入batch norm layer，

---

## 激活函数
### 1. Sigmoid
- 饱和性：两端饱和区，梯度消失
- 偏移：输出均大于0
### 2. tanh
- 解决偏移：均值为0
- 饱和性：仍会梯度消失
### 3. ReLU
- 针对饱和性：x>0无梯度消失
- x<0：神经元死亡
- 偏移问题